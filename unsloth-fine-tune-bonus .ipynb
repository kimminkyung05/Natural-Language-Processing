{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ofUvyjBlbiT"
      },
      "source": [
        "### Fine Tuning Homework with Unsloth\n",
        "\n",
        "Please refer to: https://docs.unsloth.ai/get-started/fine-tuning-llms-guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqOCqTgYlbiT"
      },
      "source": [
        "# 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi9KbqnClbiU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfAOEN23wWcO"
      },
      "source": [
        "Plz mount your google drive.\n",
        "\n",
        "And upaload this notebook on the drive's foloder 'fine-tune-tutorial'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-LuAN4bwVqQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOZfDv3ZRjoS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RXeq4_-yp-N"
      },
      "outputs": [],
      "source": [
        "work_path = '/gdrive/My Drive/fine-tune-tutorial'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibKAp4udaB48"
      },
      "source": [
        "NOTE: you can modify the folder as you want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig7OKLP2lbiU"
      },
      "source": [
        "### Specify the pretrained model as `LLama-3.1-8B-Instruct-bnb-4bit``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "LoRA Setup: Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "\n",
        "# 1: Data preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4_dG-m0Cz4"
      },
      "source": [
        "Use the given dataset `order_analysis-dataset.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nITJ0YPR42G"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from the JSON file\n",
        "dataset = load_dataset(\"json\", data_files=f\"{work_path}/order_analysis-dataset.json\", split='train')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw_Ch7uaiLLU"
      },
      "source": [
        "##  Q1: Split the dataset into training and test sets using an 80:20 ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9O5GFmGip5F"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.train_test_split(\n",
        "    test_size=0.2,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxUEqfszar2D"
      },
      "source": [
        "### 원본 데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTQR4jrDMcJf"
      },
      "outputs": [],
      "source": [
        "print(dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii25bE7TSQGn"
      },
      "outputs": [],
      "source": [
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qotYyh-Ra14l"
      },
      "source": [
        "### Q2: Transform dataset's format int ChatML\n",
        "\n",
        "instruction -> system\n",
        "\n",
        "input -> user\n",
        "\n",
        "output -> assistant\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": input},\n",
        "    {\"role\": \"assistant\", \"content\": output}\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGWtiV_WR4py"
      },
      "outputs": [],
      "source": [
        "def convert_to_chatml_format(examples):\n",
        "    systems = examples['instruction']\n",
        "    inputs = examples['input']\n",
        "    outputs = examples['output']\n",
        "\n",
        "    texts = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": s},\n",
        "            {\"role\": \"user\", \"content\": i},\n",
        "            {\"role\": \"assistant\", \"content\": o},\n",
        "        ]\n",
        "        for s, i, o in zip(systems, inputs, outputs)\n",
        "    ]\n",
        "\n",
        "    return {\"conversations\": texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuWUdJe2R9GR"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(convert_to_chatml_format, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LweR7uITjfDE"
      },
      "source": [
        "You can see that the messages are organized under the 'conversations' key for every sample in the dataset.\n",
        "\n",
        "```json\n",
        "'conversations': [\n",
        "    {'content': '너는 사용자가 입력한 주문 문장을 분석하는 에이전트이다. 주문으로부터 이를 구성하는 음식명, 옵션명, 수량을 차례대로 추출해야 한다.',\n",
        "   'role': 'system'},\n",
        "    {'content': '주문 문장: 아인슈페너 레귤러 사이즈로 부탁드리고, 콩국수 한 그릇 주세요.', 'role': 'user'},\n",
        "    {'content': '- 분석 결과 0: 음식명:아인슈페너,옵션:레귤러,수량:1\\n- 분석 결과 1: 음식명:콩국수,수량:한 그릇',\n",
        "   'role': 'assistant'}]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud3M0fMvR-RV"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGnt59lYb1t_"
      },
      "source": [
        "### Q3: Apply `apply_chat_template`\n",
        "\n",
        "Roles:\n",
        "- Special tokens representing system, user, and assistant were added.\n",
        "- These special tokens are implemented differently across various models, so a unified interface, apply_chat_template, is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxrJicpdSEDC"
      },
      "outputs": [],
      "source": [
        "from unsloth import apply_chat_template\n",
        "\n",
        "dataset = apply_chat_template(dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMjAZ2DUcPdT"
      },
      "source": [
        "You can see the special tokes added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvY8nF7Qk9tu"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhFtEAROxfD-"
      },
      "outputs": [],
      "source": [
        "tokenizer.apply_chat_template(dataset['train']['conversations'][0], tokenize = False, add_generation_prompt = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "#2: Training the Model\n",
        "The model is trained using Hugging Face TRL's `SFTTrainer`.\n",
        "\n",
        "For more detailed information, please refer to the TRL SFT docs.\n",
        "\n",
        "Due to time constraints, we will only run 60 steps.\n",
        "\n",
        "When performing actual fine-tuning:\n",
        "- Set num_train_epochs=1 or more.\n",
        "- Set max_steps=None."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset['train'].select(range(1000)),\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 512,\n",
        "    packing = True, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 2,\n",
        "        #max_steps = 60, None으로 처리하여 epoch 차이를 확인\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 1e-4,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "# 3: Applying trained model\n",
        "\n",
        "Apply the trained model to check if it has been successfully fine-tuned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2NMZUjnU6Ja"
      },
      "outputs": [],
      "source": [
        "system_message = '너는 사용자가 입력한 주문 문장을 분석하는 에이전트이다. 주문으로부터 이를 구성하는 음식명, 옵션명, 수량을 차례대로 추출해야 한다.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": \"짜장면 2그릇, 콜라 1병 주세요.\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 512, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "# 4: Saving and Loading the Fine-Tuned Model\n",
        "We only save the LoRA adapter (for efficient storage):\n",
        "- Save to the Hugging Face Hub: `push_to_hub`\n",
        "- Save locally: `save_pretrained`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(f\"{work_path}/lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(f\"{work_path}/lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "# 5: Loading the saved model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8jFjojg4HEP"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = f\"{work_path}/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 256, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "We can also load the saved model as Hugging Face style\n",
        "- `AutoModelForPeftCausalLM`.\n",
        "- `AutoTokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        f\"{work_path}/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"{work_path}/lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKcg76IInY2e"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFWkPXZdnnNX"
      },
      "source": [
        "# 6: Calculate Metrics\n",
        "\n",
        "### Q4: Complte the code for calculating BLEU score on the test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHUh6NM9nth7"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "bleu_metric = evaluate.load(\"bleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_XONILgvIER"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset['test']\n",
        "\n",
        "generated_responses = []\n",
        "reference_responses = []\n",
        "\n",
        "for i, example in enumerate(test_dataset):\n",
        "    if i >= 100:\n",
        "        break\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        example[\"conversations\"][:-1],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    generated_text = tokenizer.decode(\n",
        "        outputs[0][input_length:],\n",
        "        skip_special_tokens=True,\n",
        "    ).strip()\n",
        "\n",
        "    reference_text = example[\"output\"].strip()\n",
        "\n",
        "    generated_responses.append(generated_text.strip())\n",
        "    reference_responses.append(reference_text.strip()) # Use the original output as reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4Zh1DKtvPRl"
      },
      "outputs": [],
      "source": [
        "result_1 = bleu_metric.compute(predictions=generated_responses, references=reference_responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryhcJWigvUD4"
      },
      "outputs": [],
      "source": [
        "print(result_1['bleu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbWKHEVSv9xy"
      },
      "source": [
        "# 7: BONUS Question (*)\n",
        "\n",
        "### Q5: Try additional efforts to get the best bleu score on the test split (Plz submit another notebook for this Bonus question)\n",
        "- Option 1: Train more epochs\n",
        "- Option 2: Try other models rather than Meta-Llama-3.1-8B-Instruct-bnb-4bit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70i_j4uciC62"
      },
      "source": [
        "# - epoch 조정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Z4VP9XFK_E"
      },
      "source": [
        "**- 환경 변수 변경**\n",
        "\n",
        "Colab 환경의 자원 제약을 고려하며, BLEU 성능 향상을 목표로 추가 미세조정을 수행하였다. GPU 메모리를 절약하기 위해 batch size는 2에서 1로 줄였으며, gradient accumulation은 4에서 1로 축소하였다. 또한 warmup step은 5에서 2로 낮추어 전체 연산량을 줄였고, 학습 시간이 과도하게 길어지는 것을 방지하기 위해 max_steps는 제거하였다. 과적합을 최소화하기 위해 learning rate는 2e-4에서 1e-4로 조정하였다.\n",
        "\n",
        "이러한 설정을 고정한 상태에서 num_train_epochs = 1, 2, 3으로 순차적으로 실험을 진행하여 epoch 증가가 BLEU 성능에 미치는 영향을 분석하였다. 모든 실험은 max_seq_length = 512, packing = True로 동일하게 진행했다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLRpYT2LIco6"
      },
      "source": [
        "**- BLEU Score Results**\n",
        "1. epoch = 1인 상황에서 bleu 점수는 :0.9150880591680964\n",
        "2. epoch = 2인 상황에서 bleu 점수는 :0.9210181790206565\n",
        "3. epoch = 3인 상황에서 bleu 점수는 :0.9064429358819918"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6_BHaNM9uIX"
      },
      "source": [
        "성능 : epoch 2 > epoch1 > epoch 3\n",
        "\n",
        "Epoch을 1에서 2로 증가시켰을 때 BLEU가 상승한 것은 모델이 파인튜닝을 적절히 수행한 것으로 보인다. 그러나 epoch을 3까지 늘리자 오히려 점수가 하락했으며, 이는 과도한 반복 학습으로 인해 과적합되었기 때문이다. Epoch 3에서는 다양성이 감소하고 특정 표현 방식에 지나치게 맞춰지면서 전체 성능이 떨어지는 결과를 보였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ztGrObohqIY"
      },
      "source": [
        "# - LoRA 조정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfnwd7jRiS5q"
      },
      "source": [
        "다음으로는, 성능이 최대인 epoch=2 상황에서 LoRA 조정을 통한 성능 향상을 목표로 미세조정을 수행하였다.\n",
        "r = 16 ,lora_alpha = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjAwc3cBma4D"
      },
      "source": [
        "**- BLEU Score Results**\n",
        "1. r = 8, lora_alpha = 8인 상황에서 bleu score = 0.9138349478136333\n",
        "2. r = 16 ,lora_alpha = 16인 상황에서 bleu score = 0.9210181790206565\n",
        "3. r = 32, lora_alpha = 32인 상황에서 bleu score =0.9222873616581291\n",
        "4. r = 64, lora_alpha = 64인 상황에서 bleu score =0.9173199272743199"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8QG5VLuXac8"
      },
      "source": [
        "성능 : r= 32 > r= 16 > r= 64 > r= 8\n",
        "\n",
        "LoRA Rank를 8, 16, 32, 64로 변화시키며 성능을 비교한 결과, r이 증가할수록 모델의 표현력이 확장되어 r=32까지는 성능이 지속적으로 향상되었다. 그러나 r=64에서는 점수가 하락했는데, 이는 학습 가능한 파라미터가 늘어나 데이터의 패턴 등이 적합되었기 때문이다. 즉, LoRA는 적정 범위(약 16~32)에서는 성능을 개선하지만, 이를 초과하면 일반화 성능이 떨어져 오히려 성능이 감소하는 경향을 보이며, 본 실험에서는 r=32가 가장 효율적인 설정으로 나타났다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}